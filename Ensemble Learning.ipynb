{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
    "\"\"\"\n",
    "Ensemble Learning is a technique that combines multiple machine learning models to improve overall prediction accuracy and robustness.\n",
    "It leverages the strengths of different models so that their combined output performs better than any single model.\n",
    "\n",
    "Types:\n",
    "    -Bagging\n",
    "    -Boosting\n",
    "    -Stacking\n",
    "    -Voting\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is the difference between Bagging and Boosting?\n",
    "\"\"\"\n",
    "Bagging trains multiple models independently and in parallel on different random subsets of the data created through sampling\n",
    "with replacement. Its main goal is to reduce variance and prevent overfitting. In contrast, Boosting trains models sequentially,\n",
    "where each new model focuses on correcting the errors made by the previous ones. Boosting aims to reduce bias and improve \n",
    "overall accuracy. Common examples of Bagging include Random Forest, while AdaBoost, Gradient Boosting, and XGBoost are popular \n",
    "Boosting methods.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
    "\"\"\"\n",
    "Bootstrap sampling is a statistical technique where multiple random samples are drawn from the original dataset with \n",
    "replacement. This means the same data point can appear more than once in a sample, while some points might not appear at all.\n",
    "\n",
    "In Bagging methods like Random Forest, bootstrap sampling is used to create different training subsets for each individual \n",
    "model (e.g., each decision tree). This ensures that every model learns from a slightly different version of the data, \n",
    "introducing diversity among the models. The combined predictions from these diverse models help to reduce variance, prevent \n",
    "overfitting, and improve overall model stability and accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
    "\"\"\"\n",
    "Out-of-Bag (OOB) samples are the data points that are not included in a particular bootstrap sample when training a model in \n",
    "Bagging methods like Random Forest. Since each tree in the ensemble is trained on a random subset of the data (with replacement), roughly one-third of the data is left out and serves as OOB samples for that tree.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c58500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
    "\"\"\"\n",
    "In a single Decision Tree, feature importance is determined by how much each feature contributes to reducing impurity\n",
    "(such as Gini impurity or entropy) during the splits. The higher the reduction in impurity caused by a feature across all its \n",
    "splits, the more important that feature is considered. However, since a single tree can be sensitive to small changes in data, \n",
    "its feature importance scores may not be very stable or reliable.\n",
    "\n",
    "In contrast, a Random Forest calculates feature importance by averaging the importance scores of each feature across all the \n",
    "trees in the ensemble. This aggregation makes the feature importance values more robust, stable, and reliable, as it reduces \n",
    "the effect of randomness and overfitting that might occur in a single tree.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c0268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aditi\\anaconda3\\py\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\aditi\\anaconda3\\py\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\aditi\\anaconda3\\py\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Important Features:\n",
      "                 Feature  Importance\n",
      "23            worst area    0.139357\n",
      "27  worst concave points    0.132225\n",
      "7    mean concave points    0.107046\n",
      "20          worst radius    0.082848\n",
      "22       worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "# 6. Write a Python program to:\n",
    "# ● Load the Breast Cancer dataset using\n",
    "# sklearn.datasets.load_breast_cancer()\n",
    "# ● Train a Random Forest Classifier\n",
    "# ● Print the top 5 most important features based on feature importance scores.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "print(feature_importance_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315303a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 7. Write a Python program to:\n",
    "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                        n_estimators=50,\n",
    "                        random_state=42)\n",
    "bag.fit(X_train, y_train)\n",
    "y_pred_bag = bag.predict(X_test)\n",
    "bag_acc = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", dt_acc)\n",
    "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af99b2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'n_estimators': 150}\n",
      "Final Accuracy: 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "# 8.  Write a Python program to:\n",
    "# ● Train a Random Forest Classifier\n",
    "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
    "# ● Print the best parameters and final accuracy\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Final Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3fa4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 0.2579153056796594\n",
      "Random Forest Regressor MSE: 0.25638991335459355\n"
     ]
    }
   ],
   "source": [
    "# 9. Write a Python program to:\n",
    "# ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
    "# Housing dataset\n",
    "# ● Compare their Mean Squared Errors (MSE)\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
    "                           n_estimators=50,\n",
    "                           random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bag = bagging.predict(X_test)\n",
    "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Bagging Regressor MSE:\", mse_bag)\n",
    "print(\"Random Forest Regressor MSE:\", mse_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. You are working as a data scientist at a financial institution to predict loan\n",
    "# default. You have access to customer demographic and transaction history data.\n",
    "# You decide to use ensemble techniques to increase model performance.\n",
    "# Explain your step-by-step approach to:\n",
    "# ● Choose between Bagging or Boosting\n",
    "# ● Handle overfitting\n",
    "# ● Select base models\n",
    "# ● Evaluate performance using cross-validation\n",
    "# ● Justify how ensemble learning improves decision-making in this real-world\n",
    "# context.\n",
    "\"\"\"\n",
    "1.Choose between Bagging or Boosting:\n",
    "    -Use Bagging (e.g., Random Forest) if base models have high variance and you want to reduce overfitting.\n",
    "    -Use Boosting (e.g., XGBoost, LightGBM) if base models have high bias and you want to improve accuracy by correcting errors sequentially.\n",
    "\n",
    "2. Handle Overfitting:\n",
    "    -Use cross-validation, early stopping, and regularization (e.g., depth limits, learning rate).\n",
    "    -Apply feature selection, data balancing, and pruning to prevent overfitting.\n",
    "\n",
    "3. Select Base Models:\n",
    "    -Common base models: Decision Trees, Logistic Regression, or Weak Learners.\n",
    "    -Use diverse models in stacking for better generalization.\n",
    "\n",
    "4. Evaluate Performance (Cross-Validation):\n",
    "    -Use Stratified K-Fold CV for balanced evaluation.\n",
    "    -Metrics: ROC-AUC, Precision-Recall, F1-score, and calibration.\n",
    "\n",
    "5. Ensemble Learning Justification:\n",
    "    -Combines multiple models to reduce variance and bias, giving more accurate, stable, and reliable predictions.\n",
    "    -In loan default prediction, this improves risk assessment, reduces losses, and supports better credit decisions.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
