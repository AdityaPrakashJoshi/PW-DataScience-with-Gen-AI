{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.What is a Decision Tree, and how does it work in the context of\n",
    "# classification?\n",
    "\"\"\"\n",
    "A Decision Tree is a supervised learning algorithm that splits data into branches based on feature conditions to classify \n",
    "samples into categories. It works by recursively selecting the best feature using metrics like Gini impurity or Information\n",
    "Gain until a decision (class label) is reached at the leaf node.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2adadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. : Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
    "# How do they impact the splits in a Decision Tree?\n",
    "\"\"\"\n",
    "Gini Impurity measures how often a randomly chosen sample would be incorrectly classified if it were randomly labeled according\n",
    "to the class distribution in a node — lower Gini means purer nodes.\n",
    "Entropy measures the amount of uncertainty or disorder in a node — higher entropy means more mixed classes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
    "# Trees? Give one practical advantage of using each\n",
    "\"\"\"\n",
    "Pre-Pruning stops the tree from growing too deep during training by setting limits like maximum depth or minimum samples\n",
    "per split — this prevents overfitting early and saves computation.\n",
    "Post-Pruning allows the tree to grow fully, then removes branches that don’t improve accuracy — this helps simplify the \n",
    "model while keeping strong predictive performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "\"\"\"\n",
    "Information Gain measures how much a feature reduces the uncertainty (entropy) in the dataset after a split. \n",
    "It is important because the Decision Tree selects the feature with the highest Information Gain at each node — meaning that \n",
    "split provides the most informative and pure separation of classes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c72c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "\"\"\"\n",
    "Applications: Decision Trees are widely used in loan approval, medical diagnosis, customer churn prediction, and fraud detection.\n",
    "Advantages: They are easy to interpret, handle both numerical and categorical data, and require little data preprocessing.\n",
    "Limitations: They can overfit easily, are sensitive to small data changes, and may become biased toward features with many categories.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f471fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aditi\\anaconda3\\py\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\aditi\\anaconda3\\py\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\aditi\\anaconda3\\py\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0167\n",
      "petal length (cm): 0.9061\n",
      "petal width (cm): 0.0772\n"
     ]
    }
   ],
   "source": [
    "# 6. Write a Python program to:\n",
    "# ● Load the Iris Dataset\n",
    "# ● Train a Decision Tree Classifier using the Gini criterion\n",
    "# ● Print the model’s accuracy and feature importances\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "for name, importance in zip(iris.feature_names, model.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439925b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Fully-Grown Tree: 1.0\n",
      "Accuracy of Tree with max_depth=3: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 7. Write a Python program to:\n",
    "# ● Load the Iris Dataset\n",
    "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
    "# a fully-grown tree.\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_full = DecisionTreeClassifier(random_state=42)\n",
    "model_full.fit(X_train, y_train)\n",
    "y_pred_full = model_full.predict(X_test)\n",
    "acc_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "model_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "model_limited.fit(X_train, y_train)\n",
    "y_pred_limited = model_limited.predict(X_test)\n",
    "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
    "\n",
    "print(\"Accuracy of Fully-Grown Tree:\", acc_full)\n",
    "print(\"Accuracy of Tree with max_depth=3:\", acc_limited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf65904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 10.416078431372549\n",
      "CRIM: 0.0513\n",
      "ZN: 0.0034\n",
      "INDUS: 0.0058\n",
      "CHAS: 0.0000\n",
      "NOX: 0.0271\n",
      "RM: 0.6003\n",
      "AGE: 0.0136\n",
      "DIS: 0.0707\n",
      "RAD: 0.0019\n",
      "TAX: 0.0125\n",
      "PTRATIO: 0.0110\n",
      "B: 0.0090\n",
      "LSTAT: 0.1933\n"
     ]
    }
   ],
   "source": [
    "# 8. : Write a Python program to:\n",
    "# ● Load the Boston Housing Dataset\n",
    "# ● Train a Decision Tree Regressor\n",
    "# ● Print the Mean Squared Error (MSE) and feature importances\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "for name, importance in zip(boston.feature_names, model.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0996381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 9. Write a Python program to:\n",
    "# ● Load the Iris Dataset\n",
    "# ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
    "# GridSearchCV\n",
    "# ● Print the best parameters and the resulting model accuracy\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92403599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10c.Imagine you’re working as a data scientist for a healthcare company that\n",
    "# wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "# mixed data types and some missing values.\n",
    "# Explain the step-by-step process you would follow to:\n",
    "# ● Handle the missing values\n",
    "# ● Encode the categorical features\n",
    "# ● Train a Decision Tree model\n",
    "# ● Tune its hyperparameters\n",
    "# ● Evaluate its performance\n",
    "# And describe what business value this model could provide in the real-world\n",
    "# setting.\n",
    "\n",
    "Imagine you’re working as a data scientist for a healthcare company that\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "Explain the step-by-step process you would follow to:\n",
    "● Handle the missing values\n",
    "● Encode the categorical features\n",
    "● Train a Decision Tree model\n",
    "● Tune its hyperparameters\n",
    "● Evaluate its performance\n",
    "And describe what business value this model could provide in the real-world\n",
    "setting.\n",
    "\"\"\"1. Handle Missing Values\n",
    "\n",
    "Identify missing values in the dataset using methods like isnull() or info().\n",
    "\n",
    "Numerical features: Impute missing values using mean or median.\n",
    "\n",
    "Categorical features: Impute missing values using the mode or a placeholder like 'Unknown'.\n",
    "\n",
    "Optional: Use advanced methods like K-Nearest Neighbors imputation for more accuracy.\n",
    "\n",
    "2. Encode Categorical Features\n",
    "\n",
    "Convert categorical variables into numerical form so that the Decision Tree can process them.\n",
    "\n",
    "Label Encoding: For ordinal categories (e.g., 'low', 'medium', 'high').\n",
    "\n",
    "One-Hot Encoding: For nominal categories (e.g., 'blood type').\n",
    "\n",
    "3. Train a Decision Tree Model\n",
    "\n",
    "Split the dataset into training and testing sets (e.g., 80/20 split).\n",
    "\n",
    "Initialize a Decision Tree Classifier using sklearn.tree.DecisionTreeClassifier.\n",
    "\n",
    "Fit the model on the training data.\n",
    "\n",
    "4. Tune Hyperparameters\n",
    "\n",
    "Use GridSearchCV or RandomizedSearchCV to find the best hyperparameters:\n",
    "\n",
    "max_depth → prevents overfitting by limiting tree depth.\n",
    "\n",
    "min_samples_split → minimum samples required to split a node.\n",
    "\n",
    "criterion → Gini or Entropy to measure node purity.\n",
    "\n",
    "Select the best model based on cross-validated performance.\n",
    "\n",
    "5. Evaluate Performance\n",
    "\n",
    "Use the testing set to evaluate the model.\n",
    "\n",
    "Common metrics for classification:\n",
    "\n",
    "Accuracy → overall correctness.\n",
    "\n",
    "Precision → proportion of predicted positives that are correct (important in healthcare to avoid false positives).\n",
    "\n",
    "Recall (Sensitivity) → proportion of actual positives correctly identified (critical to avoid missing patients with the disease).\n",
    "\n",
    "F1-Score → balances precision and recall.\n",
    "\n",
    "ROC-AUC → evaluates model discrimination ability.\n",
    "\n",
    "6. Business Value\n",
    "\n",
    "Early Detection: Helps doctors identify patients at risk earlier, improving treatment outcomes.\n",
    "\n",
    "Resource Optimization: Focus medical resources on high-risk patients.\n",
    "\n",
    "Decision Support: Assists healthcare professionals in making data-driven decisions.\n",
    "\n",
    "Cost Reduction: Reduces unnecessary tests for low-risk patients while prioritizing high-risk ones.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
